---
title: Causal Inference Homework Assignment 4
subtitle: The role of propensity scores in observational study
author:
    - Alan Z Chen
    - Chansoo Song
date: "`r format(Sys.time(), '%B %Y')`"
header-includes:
    - \usepackage{amsmath}
output: pdf_document
---

```{r rmd_setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
knitr::opts_chunk$set(include=TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60), tidy=TRUE)
knitr::opts_knit$set(root.dir='..')
```

```{r rmd, message=FALSE}
library(data.table)
library(arm)
library(ggplot2)
library(rlang)
library(RColorBrewer)
library(stats)
library(caret)
library(randomForest)
library(nnet)
library(dplyr)

rm(list=ls())
# Set global ggplot theme
theme_set(theme_minimal())
# Define global color palettes
col.RdBl.2 <- brewer.pal(3, 'RdBu')[-2]
```


# Objective  
This assignment will give you the opportunity to practice several different propensity score approaches to causal inference. In addition you will be asked to interpret the resulting output and discuss the assumptions necessary for causal inference.


# Problem Statement  
In this assignment will use data from a constructed observational study. The data and an associated data dictionary are available in this folder. The treatment group for the study that the data are drawn from is the group of children who participated in the IHDP intervention discussed in class. The research question of interest focuses on the effect of the IHDP intervention on age 3 IQ scores for the children that participated in it. The data for the comparison sample of children was pulled from the National Longitudinal Study of Youth during a similar period of time that the data were collected for the IHDP study.


## Question 1: Load the data and choose confounders (Step 1)

\texttt{ANSWER:}
```{r Q1}
load('data/hw4.rdata')
hw4 <- data.table(hw4)

# Pick confounders based on correlation to treatment and outcome variables
corr_mat = cor(hw4)

corr_mat = rbind(corr_mat['treat',],corr_mat['ppvtr.36',])
confounders = names(which(apply(abs(corr_mat) >= 0.3 & abs(corr_mat) != 1, 2, sum) > 0))
confounders = confounders[confounders != 'st99']

# Subset data for analysis
dt <- hw4[bw<3000, c('ppvtr.36', 'treat', confounders), with=FALSE]
```


## Question 2: Estimate the propensity score (Step 2)

\texttt{ANSWER:}
```{r Q2}
ps.m1 <- glm(treat ~ ., data=dt[,c('treat',confounders),with=F], family=binomial(link='logit'))
dt$psc <- ps.m1$fitted.values
```


## Question 3: Restructure your data through matching. [Or at least create the weights variable that will let you to do so in the following steps] (Step 3)

- (a) The first thing you need to be clear on before restructuring your data is the estimand. Given the description above about the research question, what is the estimand of interest?  
- (b) First please perform *one-to-one nearest neighbor matching with replacement* using your estimated propensity score from Question 2. Perform this matching using the `matching` command in the arm package. The *"cnts"* variable in the output reflects the number of times each control observation was used as a match (the length is equal to the number of control observations). Use the output of this function to create a weight variable that  
- 1) equals one for treated observations and  
- 2) equals the number of times used as a match for non- treated observations.  

\texttt{ANSWER:}  
The IHDP intervention targeted children that were born premature and with low birth weight, thus the estimand of interest is the \texttt{ATT} (average treatment effect for the treated).

```{r Q3}
matches <- matching(z=dt$treat, score=dt$psc, replace=TRUE)
wts <- matches$cnts
```


## Question 4: Check overlap and balance. (Step 4)

### (a) Examining Overlap. Check overlap on the *unmatched* data using some diagnostic plots. Check overlap for the propensity scores as well as two other covariates.

\texttt{ANSWER:}
```{r Q4a}
# Paired, inverted histograms: propensity scores
examine_overlap = function(dt, nbins=40, var='psc', var_name='Propensity Score', ylim=c(-30,50), xlab='Propensity Score', options=NULL){
    dt_plot = dt[, c('treat',var), with=F]
    colnames(dt_plot) = c('treat','X')
    ggplot(dt_plot) +
    geom_histogram(data=dt_plot[treat==0,], bins=nbins, fill='grey', alpha=0.1, aes(X, y=..count.., color=col.RdBl.2[2])) +
    geom_histogram(data=dt_plot[treat==1,], bins=nbins, fill='grey', alpha=0.1, aes(X, y=-..count.., color=col.RdBl.2[1])) +
    coord_cartesian(ylim=ylim) +
    scale_color_manual(values=rev(col.RdBl.2), labels=c('Control', 'Treated'), name='') +
    labs(x=xlab, y='Frequency', title=paste('Overlap of ',var_name,' between Groups',sep='')) +
    options 
}

examine_overlap(dt, 40, var='psc', var_name='Propensity Score', ylim=c(-30,50), xlab='Propensity Score')
examine_overlap(dt, 20, var='preterm', var_name='Number Weeks Baby Born Preterm', ylim=c(-100,150), xlab='Weeks Preterm')
examine_overlap(dt, 20, var='bw', var_name='Birthweight', ylim=c(-50,200), xlab='Birthweight')
```

### (b) Interpreting Overlap. What do these plots reveal about the overlap required to estimate our estimand of interest.

\texttt{ANSWER:}  
Since our estimand of interest is the \texttt{ATT}, we need similar observations from the control group to use as our counterfactuals for the treatment group. The plots above show sufficient overlap (relative to the treated) between the two groups in propensity scores, mother's education level, and number of weeks the baby was born preterm. There is overlap in mother's age at birth from approximately around age values 25-35, but there are no counterfactual observations for the treated children with mothers that are either younger or older than that 25-35 age range.

### (c) Examining Balance. You will build your own function to check balance! This function should take as inputs the data frame created in Question 1, the vector with the covariate names chosen in Question 1, and the weights created in Question 2. It should output the following:  

- 1) Mean in the unmatched treatment group  
- 2) Mean in the unmatched control group  
- 3) Mean in the matched treatment group  
- 4) Mean in the matched control group  
- 5) Unmatched mean difference (standardized for continuous variables, not standardized for binary variables)  
- 6) Matched mean difference (standardized for continuous variables, not standardized for binary variables)  
- 7) Ratio of standard deviations across unmatched groups (control/treated)  
- 8) Ratio of standard deviations across matched groups (control/treated)

\texttt{ANSWER:}
```{r Q4c}
isBinary <- function(x) {
    # Check if x is binary
    all(x %in% c(0,1))
}

wtdMean <- function(x, wts) {
    # Calculate weighted mean of x
    sum(wts * x) / sum(wts)
}

wtdVar <- function(x, wts) {
    # Calculate weighted sample variance
    if (isBinary(x)) {
        sum(wts^2 * mean(x) * (1 - mean(x))) / sum(wts)
    } else {
        sum(wts * (x - wtdMean(x, wts))^2) / (sum(w) - 1)
    }
}

meanDiff <- function(x, data, wtd=FALSE) {
    # Calculate mean difference of x between treated and controls
    # Standardized mean difference for continuous variables
    treated <- data[treat==1, get(x)]
    control <- data[treat==0, get(x)]

    if (wtd) {
        wts <- data[treat==0, w]
    } else {
        wts <- rep(1, length(control))
    }

    x1 <- mean(treated)
    x0 <- wtdMean(control, wts)

    if (isBinary(data[, get(x)])) {
        x1 - x0
    } else {
        var1 <- var(treated)
        if (length(wts) == 1) {
            var0 <- wtdVar(control, wts)
        } else {
            var0 <- var(control)
        }
        (x1 - x0) / sqrt((var1 + var0) / 2)
    }
}


ratioSD <- function(x, data, wtd=FALSE) {
    # Calculate ratio (control/treated) of standard deviations
    treated <- data[treat==1, get(x)]
    control <- data[treat==0, get(x)]
    x.binary <- isBinary(data[, get(x)])

    if (wtd) {
        wts <- data[treat==0, w]
        if (x.binary) {
            var1 <- mean(treated) * (1 - mean(treated))
            # var0 <- mean(control) * (1 - mean(control))
            var0 <- wtd.var(control, wts)
        } else {
            var1 <- var(treated)
            var0 <- wtdVar(control, wts)
        }
    } else {
        if (x.binary) {
            var1 <- mean(treated) * (1 - mean(treated))
            var0 <- mean(control) * (1 - mean(control))
        } else {
            var1 <- var(treated)
            var0 <- var(control)
        }
    }
    sqrt(var0) / sqrt(var1)
}

checkBalance <- function(data, X, wts) {
    # set weights
    data[treat==0, w:=wts]
    data[treat==1, w:=1]

    # 1. Means in unmatched treatment group
    mn1 <- melt(data[treat==1, sapply(.SD, mean), .SDcols=X], value.name='mn1')
    # 2. Means in unmatched control group
    mn0 <- melt(data[treat==0, sapply(.SD, mean), .SDcols=X], value.name='mn0')
    # 3. Means in matched treatment group
    mn1.m <- melt(data[treat==1 & w>0, sapply(.SD, mean), .SDcols=X], value.name='mn1.m')
    # 4 Means in matched control group
    mn0.m <- melt(data[treat==0 & w>0, sapply(.SD, wtdMean, wts=w), .SDcols=X], value.name='mn0.m')
    # 5. Mean difference in unmatched sample
    diff <- melt(sapply(X, meanDiff, data=data))
    # 6. Mean difference in matched sample
    diff.m <- melt(sapply(X, meanDiff, data=data[w>0,], wtd=TRUE))
    # 7. Ratio of standard deviation in unmatched sample (control/treated)
    ratio <- melt(sapply(X, ratioSD, data=data), value.name='ratio')
    # 8. Ratio of standard deviation in matched sample (control/treated)
    ratio.m <- melt(sapply(X, ratioSD, data=data[w>0,], wtd=TRUE), value.name='ratio.m')
}
```

### (d) How do you interpret the resulting balance? In particular what are your concerns with regard to covariates that are not well balanced (write about 5 or 6 sentences).

\texttt{ANSWER:}
```{r Q4d}
df = check_balance(dt, confounders, wts)
as.matrix(df)
```

### (e) Unit test. Show the results of your balance function on a simple example with the same sample as above (that is, limited to children with birth weight less than 3000) where the propensity score is fit using logistic regression on “bw” and “b.marr” and the matching is performed using 1-1 nearest neighbor matching with replacement. The output of your balance function should match the following (when rounded to 3 decimal places):

\texttt{ANSWER:}
```{r Q4e}
# Subset data
temp <- hw4[bw<3000, .(treat, bw, b.marr)]
# Fit propensity score model (logistic)
ps.m2 <- glm(treat ~ bw + b.marr, data=temp, family=binomial(link='logit'))
# Generate propensities scores
temp$psc <- ps.m2$fitted.values
# 1-1 nearest neighbor matching with replacement
ps.m2.matches <- matching(z=temp$treat, score=temp$psc, replace=TRUE)

temp[treat==0, w:=ps.m2.matches$cnts]
temp[treat==1, w:=1]

mean(temp[treat==0, bw])
mean(temp[treat==0 & w==0, bw])

check_balance(temp, c('bw', 'b.marr'), ps.m2.matches$cnts)
```



## Question 5: Repeat steps 2-4 within the matching framework.

\texttt{ANSWER:}
```{r Q5}
ps.m2 <- glm(treat ~ ., data=dt[,c('treat',confounders),with=F], family=binomial(link='probit'))
summary(ps.m2)
dt$psc_probit <- ps.m2$fitted.values
confusionMatrix(ifelse(dt$psc_probit>0.5,1,0), dt$treat)

ps.m3 <- glm(treat ~ .^2, data=dt[,c('treat',confounders),with=F], family=binomial(link='probit'))
summary(ps.m3)
dt$psc_full <- ps.m3$fitted.values
confusionMatrix(ifelse(dt$psc_full>0.5,1,0), dt$treat)

m.rf = randomForest(as.factor(treat) ~ ., data=dt[,c('treat',confounders),with=F], ntree = 10)
dt$psc_rf = predict(m.rf, dt, type="prob")[,2]
confusionMatrix(ifelse(dt$psc_rf>0.5,1,0), dt$treat)

dt_nnet = as.data.frame(dt[,c('treat',confounders),with=F])
dt_nnet[,which(sapply(dt_nnet,class)=="numeric")] = scale(dt_nnet[,which(sapply(dt_nnet,class)=="numeric")])
dt_nnet$treat = as.factor(dt_nnet$treat)

nn_mod = nnet(treat ~ ., data=dt_nnet, size=8, linout = F, maxit=200)
dt$psc_nn = predict(nn_mod)
confusionMatrix(ifelse(dt$psc_nn>0.5,1,0), dt$treat)
```

```{r}
# Mahalo
myMH = function(trtnms, ctrnms, inv.cov, data){
  covars = dimnames(inv.cov)[[1]]
  xdiffs = as.matrix(data[trtnms,covars])
  xdiffs = xdiffs - as.matrix(data[ctrnms, covars])
  sqrt(rowSums((xdiffs %*% inv.cov) * xdiffs))
}

dt_mahalo = as.data.frame(dt)
inv_cov_mat = solve(cov(dt_mahalo[,confounders]))
trtnms = row.names(dt_mahalo[as.logical(dt_mahalo$treat),])
ctrnms = row.names(dt_mahalo[!as.logical(dt_mahalo$treat),])
mahalo_dist = outer(trtnms,ctrnms,FUN = myMH, inv.cov = inv_cov_mat, data = dt_mahalo)

matches = apply(mahalo_dist,1, function(x) min(which(x == min(x))))
match_results = as.data.frame(cbind(seq(1,290), matches+290))
colnames(match_results) = c('trt.idx','ctr.idx')

counts = match_results %>% 
group_by(ctr.idx) %>%
dplyr::summarize(w_mahalo=n())

dt_mahalo_2 = dt_mahalo %>%
mutate(id = as.integer(rownames(dt_mahalo))) %>%
left_join(counts, by = c("id" = "ctr.idx")) %>%
mutate(w_mahalo = treat + ifelse(is.na(w_mahalo),0,w_mahalo))

dt$w_mahalo = dt_mahalo_2$w_mahalo

# Check against MatchIt function
library(MatchIt)     
df = dt[,c('treat',confounders),with=F]
zz <- matchit(treat ~ momed + booze + bw + bwg + preterm + black + white + dayskidh, data=df, method="nearest", 
              distance="mahalanobis", replace=TRUE)
zz.out = zz$match.matrix
sum(match_results$ctr.idx == zz.out)
```

```{r}
# Rematching
matches <- matching(z=dt$treat, score=dt$psc_probit, replace=TRUE)
dt[treat==1, w_probit:=1]
dt[treat==0, w_probit:=matches$cnts]

matches <- matching(z=dt$treat, score=dt$psc_full, replace=TRUE)
dt[treat==1, w_full:=1]
dt[treat==0, w_full:=matches$cnts]

matches <- matching(z=dt$treat, score=dt$psc_rf, replace=TRUE)
dt[treat==1, w_rf:=1]
dt[treat==0, w_rf:=matches$cnts]

matches <- matching(z=dt$treat, score=dt$psc_nn, replace=TRUE)
dt[treat==1, w_nn:=1]
dt[treat==0, w_nn:=matches$cnts]

# OVERLAP
# Paired, inverted histograms: propensity scores
nbins <- 40
examine_overlap(dt,40,var='psc_probit',var_name='Propensity Score (Probit Model)',ylim=c(-30,50),xlab='Propensity Score')
examine_overlap(dt,40,var='psc_full',var_name='Propensity Score (Probit Full Model)',ylim=c(-30,50),xlab='Propensity Score')
examine_overlap(dt,40,var='psc_rf',var_name='Propensity Score (RF Model)',ylim=c(-30,50),xlab='Propensity Score')
examine_overlap(dt,40,var='psc_nn',var_name='Propensity Score (NN Model)',ylim=c(-30,50),xlab='Propensity Score')

df = check_balance(dt, confounders, wts)
as.matrix(df)

df2 = check_balance(dt, confounders, dt$w_probit[291:1320])
as.matrix(df2)

df3 = check_balance(dt, confounders, dt$w_full[291:1320])
as.matrix(df3)

df4 = check_balance(dt, confounders, dt$w_rf[291:1320])
as.matrix(df4)

df5 = check_balance(dt, confounders, dt$w_nn[291:1320])
as.matrix(df5)

df6 = check_balance(dt, confounders, dt$w_mahalo[291:1320])
as.matrix(df6)
```


## Question 6: Repeat steps 2-4, but this time using IPTW.

\texttt{ANSWER:}
```{r Q6}
# Code for Estimating the Propensity Score
ps.m1 <- glm(treat ~ ., data=dt[,c('treat',confounders),with=F], family=binomial(link='logit'))
dt$psc <- ps.m1$fitted.values

# Code for Creating the IPTW Weights
trt = dt$treat==1
ctrl = dt$treat==0
```

For estimating the ATE, define the weight as follows:
$$\omega(Z,x) = \frac{Z}{\hat{\epsilon}(x)} + \frac{(1-Z)}{1-\hat{\epsilon(x)}}$$
```{r}
r_ate = dt$treat/dt$psc + (1-dt$treat)/(1-dt$psc)
```


For estimating the ATT, define the weight as follows:
$$\omega(Z,x) = Z + (1-Z) * \frac{\hat{\epsilon}(x)}{1-\hat{\epsilon}(x)}$$
```{r}
r_att = dt$psc + (1-dt$treat) * dt$psc / (1-dt$psc)
```

```{r}
iptw.balance <- check_balance(dt, confounders, r_att)
```

### Question 7: Comparative balance table Create a table with columns 6 and 8 from your function for each of the matching and weighting methods performed above. Which approach would you choose and why? (1-2 paragraphs at most)

### Question 8: Estimate the treatment effect for the restructured datasets implied by Questions 4-6 (Step 5) 

Estimate the effect of the treatment on the treated for each of your five approaches by fitting a regression with weights equal to the number of times each observation appears in the matched sample (that is, use your weights variable from above) or using IPTW weights. Report the treatment effect and standard error for each approach.



```{r}
dt[treat==0, w:=wts]
dt[treat==1, w:=1]
df = dt[,c('ppvtr.36','treat',confounders),with=F]

mod1 = summary(lm(ppvtr.36 ~ ., data = df, weights = dt$w))$coefficients
mod1.out = c(mod1[2,1],mod1[2,2])

mod2 = summary(lm(ppvtr.36 ~ ., data = df, weights = dt$w_probit))$coefficients
mod2.out = c(mod2[2,1],mod2[2,2])

mod3 = summary(lm(ppvtr.36 ~ ., data = df, weights = dt$w_full))$coefficients
mod3.out = c(mod3[2,1],mod3[2,2])

mod4 = summary(lm(ppvtr.36 ~ ., data = df, weights = dt$w_mahalo))$coefficients
mod4.out = c(mod4[2,1],mod4[2,2])

mod5 = summary(lm(ppvtr.36 ~ ., data = df, weights = r_att))$coefficients
mod5.out = c(mod5[2,1],mod5[2,2])

q8_results = rbind(mod1.out,mod2.out,mod3.out,mod4.out,mod5.out)
row.names(q8_results) = c('logit','probit','full probit','mahalo','IPTW')
colnames(q8_results) = c('coef','se')
q8_results

plot(dt$w_probit)
plot(dt$w)
plot(dt$w_full)
plot(dt$w_mahalo)
```


### Question 9: Assumptions What assumptions are necessary to interpret the estimates from the propensity score approaches causally? List and describe briefly.

### Question 10: Causal Interpretation Provide a causal interpretation of *one* of your estimates above. Remember to specify the counterfactual and to be clear about whom you are making inferences. Also make sure to use causal (counterfactual) language.

### Question 11: Comparison to linear regression Fit a regression of your outcomes to the treatment indicator and covariates.

(a) Report your estimate and standard error.

(b) Interpret your results non-causally. (c) Why might we prefer the results from the propensity score approach to the linear regression results in terms of identifying a causal effect?