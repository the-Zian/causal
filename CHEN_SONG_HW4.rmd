---
title: Causal Inference Homework Assignment 4
subtitle: The role of propensity scores in observational study
author:
    - Alan Z Chen
    - Chansoo Song
date: "`r format(Sys.time(), '%B %Y')`"
header-includes:
  - \usepackage{amsmath}
  output: pdf_document
  ---

```{r rmd_setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
knitr::opts_chunk$set(include=TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60), tidy=TRUE)
knitr::opts_knit$set(root.dir='..')
```

```{r rmd, message=FALSE}
library(data.table)
library(arm)
library(ggplot2)
library(rlang)
library(RColorBrewer)
library(stats)
library(Hmisc)
library(caret)
library(randomForest)
library(nnet)
library(dplyr)

# Set global ggplot theme
theme_set(theme_minimal())
# Define global color palettes
col.RdBl.2 <- brewer.pal(3, 'RdBu')[-2]
```


# Objective  
This assignment will give you the opportunity to practice several different propensity score approaches to causal inference. In addition you will be asked to interpret the resulting output and discuss the assumptions necessary for causal inference.


# Problem Statement  
In this assignment will use data from a constructed observational study. The data and an associated data dictionary are available in this folder. The treatment group for the study that the data are drawn from is the group of children who participated in the IHDP intervention discussed in class. The research question of interest focuses on the effect of the IHDP intervention on age 3 IQ scores for the children that participated in it. The data for the comparison sample of children was pulled from the National Longitudinal Study of Youth during a similar period of time that the data were collected for the IHDP study.


## Question 1: Load the data and choose confounders (Step 1)

\texttt{ANSWER:}
```{r Q1}
load('data/hw4.rdata')
hw4 <- data.table(hw4)

# Pick confounders based on correlation to treatment and outcome variables
corr_mat = cor(hw4)

corr_mat = rbind(corr_mat['treat',],corr_mat['ppvtr.36',])
confounders = names(which(apply(abs(corr_mat) >= 0.3 & abs(corr_mat) != 1, 2, sum) > 0))

# Subset data for analysis
dt <- hw4[bw<3000, c('ppvtr.36', 'treat', confounders), with=FALSE]
```


## Question 2: Estimate the propensity score (Step 2)

\texttt{ANSWER:}
```{r Q2}
ps.m1 <- glm(treat ~ . - ppvtr.36, data=dt, family=binomial(link='logit'))
dt$psc <- ps.m1$fitted.values
```


## Question 3: Restructure your data through matching. [Or at least create the weights variable that will let you to do so in the following steps] (Step 3)

- (a) The first thing you need to be clear on before restructuring your data is the estimand. Given the description above about the research question, what is the estimand of interest?  
- (b) First please perform *one-to-one nearest neighbor matching with replacement* using your estimated propensity score from Question 2. Perform this matching using the `matching` command in the arm package. The *"cnts"* variable in the output reflects the number of times each control observation was used as a match (the length is equal to the number of control observations). Use the output of this function to create a weight variable that  
- 1) equals one for treated observations and  
- 2) equals the number of times used as a match for non- treated observations.  

\texttt{ANSWER:}  
The IHDP intervention targeted children that were born premature and with low birth weight, thus the estimand of interest is the \texttt{ATT} (average treatment effect for the treated).

```{r Q3}
matches <- matching(z=dt$treat, score=dt$psc, replace=TRUE)
wts <- matches$cnts
```


## Question 4: Check overlap and balance. (Step 4)

### (a) Examining Overlap. Check overlap on the *unmatched* data using some diagnostic plots. Check overlap for the propensity scores as well as two other covariates.

\texttt{ANSWER:}
```{r Q4a}
# Paired, inverted histograms: propensity scores
examine_overlap = function(dt, nbins=40, var='psc', var_name='Propensity Score', ylim=c(-30,50), xlab='Propensity Score', options=NULL){
    dt_plot = dt[, c('treat',var), with=F]
    colnames(dt_plot) = c('treat','X')
    ggplot(dt_plot) +
    geom_histogram(data=dt_plot[treat==0,], bins=nbins, fill='grey', alpha=0.1, aes(X, y=..count.., color=col.RdBl.2[2])) +
    geom_histogram(data=dt_plot[treat==1,], bins=nbins, fill='grey', alpha=0.1, aes(X, y=-..count.., color=col.RdBl.2[1])) +
    coord_cartesian(ylim=ylim) +
    scale_color_manual(values=rev(col.RdBl.2), labels=c('Control', 'Treated'), name='') +
    labs(x=xlab, y='Frequency', title=paste('Overlap of ',var_name,' between Groups',sep='')) +
    options 
}

examine_overlap(dt, 40, var='psc', var_name='Propensity Score', ylim=c(-30,50), xlab='Propensity Score')
examine_overlap(dt, 12, var='momed', var_name='Mother\'s Education', ylim=c(-150,450), xlab='Mother\'s Education', options=scale_x_continuous(labels=c('Less than HS', 'High School', 'Less than college', 'College')))
examine_overlap(dt, 20, var='preterm', var_name='Number Weeks Baby Born Preterm', ylim=c(-100,150), xlab='Weeks Preterm')
```

### (b) Interpreting Overlap. What do these plots reveal about the overlap required to estimate our estimand of interest.

\texttt{ANSWER:}  
Since our estimand of interest is the \texttt{ATT}, we need similar observations from the control group to use as our counterfactuals for the treatment group. The plots above show sufficient overlap (relative to the treated) between the two groups in propensity scores, mother's education level, and number of weeks the baby was born preterm. There is overlap in mother's age at birth from approximately around age values 25-35, but there are no counterfactual observations for the treated children with mothers that are either younger or older than that 25-35 age range.

### (c) Examining Balance. You will build your own function to check balance! This function should take as inputs the data frame created in Question 1, the vector with the covariate names chosen in Question 1, and the weights created in Question 2. It should output the following:  

- 1) Mean in the unmatched treatment group  
- 2) Mean in the unmatched control group  
- 3) Mean in the matched treatment group  
- 4) Mean in the matched control group  
- 5) Unmatched mean difference (standardized for continuous variables, not standardized for binary variables)  
- 6) Matched mean difference (standardized for continuous variables, not standardized for binary variables)  
- 7) Ratio of standard deviations across unmatched groups (control/treated)  
- 8) Ratio of standard deviations across matched groups (control/treated)

\texttt{ANSWER:}
```{r Q4c}
mean_diff <- function(x, data, matched=FALSE) {
    treat <- data[treat==1, get(x)]
    if (matched) {
        ctrl <- data[treat==0 & w>0, get(x)]
        wts <- data[treat==0 & w>0, w]
    } else {
        ctrl <- data[treat==0, get(x)]
        wts <- rep(1, length(ctrl))
    }

    mean.treat <- mean(treat)
    mean.ctrl <- weighted.mean(ctrl, wts)
    diff <- mean.treat - mean.ctrl

    # Standardize mean differences for continuous variables
    if (!all(range(data[, get(x)]) == c(0,1))) {
        sd.treat <- sd(treat)
        sd.ctrl <- sqrt(wtd.var(ctrl, wts))

        diff <- diff / sqrt(sd.treat + sd.ctrl)
    }
    return(diff)
}

sd_binary <- function(x) {
    n <- length(x)
    p <- sum(x) / n
    return(sqrt(n*p*(1-p)))
}

ratio_sd <- function(x, data, matched=FALSE) {
    treat <- data[treat==1, get(x)]
    if (matched) {
        ctrl <- data[treat==0 & w>0, get(x)]
        wts <- data[treat==0 & w>0, w]
    } else {
        ctrl <- data[treat==0, get(x)]
        wts <- rep(1, length(ctrl))
    }

    sd.treat <- sd(treat)
    sd.ctrl <- sqrt(wtd.var(ctrl, wts))
    ratio <- sd.ctrl / sd.treat

    return(ratio)
}

check_balance <- function(data, X, wts) {
    # Store weights in data
    data[treat==0, w:=wts]
    data[treat==1, w:=1]

    # 1-2. means in unmatched data
    means.unmatched <- data[, lapply(.SD, mean), by=treat, .SDcols=X]
    means.unmatched[, `:=`(matched=0, Estimate='Mean')]

    # 3-4. means in matched data
    means.matched <- data[w>0, lapply(.SD, weighted.mean, w=w), by=treat, .SDcols=X]
    means.matched[, `:=`(matched=1, Estimate='Mean')]

    # 5-6. mean differences of confounders
    mean.diffs.unmatched <- sapply(X, mean_diff, data)
    mean.diffs.unmatched$matched <- 0
    mean.diffs.matched <- sapply(X, mean_diff, data, TRUE)
    mean.diffs.matched$matched <- 1

    # 7-8. ratio of sd
    ratio.unmatched <- sapply(X, ratio_sd, data)
    ratio.unmatched$matched <- 0
    ratio.matched <- sapply(X, ratio_sd, data, TRUE)
    ratio.matched$matched <- 1

    # Store estimates
    mean.diffs <- data.table(rbind(mean.diffs.unmatched, mean.diffs.matched))
    mean.diffs[, Estimate:='Mean diff.']
    ratios <- data.table(rbind(ratio.unmatched, ratio.matched))
    ratios[, Estimate:='Ratio of stdev']
    ests <- rbind(means.matched, means.unmatched, mean.diffs, ratios, fill=TRUE)
    setcolorder(ests, c('Estimate', 'matched', 'treat', X))

    return(ests)
}

checkBalance <- function(data, cnfdrs, ws) {
    mn1
}
```

### (d) How do you interpret the resulting balance? In particular what are your concerns with regard to covariates that are not well balanced (write about 5 or 6 sentences).

\texttt{ANSWER:}
```{r Q4d}
check_balance(dt, confounders, wts)
```

### (e) Unit test. Show the results of your balance function on a simple example with the same sample as above (that is, limited to children with birth weight less than 3000) where the propensity score is fit using logistic regression on “bw” and “b.marr” and the matching is performed using 1-1 nearest neighbor matching with replacement. The output of your balance function should match the following (when rounded to 3 decimal places):

\texttt{ANSWER:}
```{r Q4e}
# Subset data
temp <- hw4[bw<3000, .(treat, bw, b.marr)]
# Fit propensity score model (logistic)
ps.m2 <- glm(treat ~ bw + b.marr, data=temp, family=binomial(link='logit'))
# Generate propensities scores
temp$psc <- ps.m2$fitted.values
# 1-1 nearest neighbor matching with replacement
ps.m2.matches <- matching(z=temp$treat, score=temp$psc, replace=TRUE)

temp[treat==0, w:=ps.m2.matches$cnts]
temp[treat==1, w:=1]

mean(temp[treat==0, bw])
mean(temp[treat==0 & w==0, bw])

check_balance(temp, c('bw', 'b.marr'), ps.m2.matches$cnts)
```


## Question 5: Repeat steps 2-4 within the matching framework.

\texttt{ANSWER:}
```{r Q5}
ps.m2 <- glm(treat ~ ., data=dt[,c('treat',confounders),with=F], family=binomial(link='probit'))
summary(ps.m2)
dt$psc_probit <- ps.m2$fitted.values
confusionMatrix(ifelse(dt$psc_probit>0.5,1,0), dt$treat)

ps.m3 <- glm(treat ~ .^2, data=dt[,c('treat',confounders),with=F], family=binomial(link='probit'))
summary(ps.m3)
dt$psc_full <- ps.m3$fitted.values
confusionMatrix(ifelse(dt$psc_full>0.5,1,0), dt$treat)

m.rf = randomForest(as.factor(treat) ~ ., data=dt[,c('treat',confounders),with=F], ntree = 10)
dt$psc_rf = predict(m.rf, dt, type="prob")[,2]
confusionMatrix(ifelse(dt$psc_rf>0.5,1,0), dt$treat)

dt_nnet = as.data.frame(dt[,c('treat',confounders),with=F])
dt_nnet[,which(sapply(dt_nnet,class)=="numeric")] = scale(dt_nnet[,which(sapply(dt_nnet,class)=="numeric")])
dt_nnet$treat = as.factor(dt_nnet$treat)

nn_mod = nnet(treat ~ ., data=dt_nnet, size=8, linout = F, maxit=200)
dt$psc_nn = predict(nn_mod)
confusionMatrix(ifelse(dt$psc_nn>0.5,1,0), dt$treat)

# Mahalo
myMH = function(trtnms, ctrnms, inv.cov, data){
    covars = dimnames(inv.cov)[[1]]
    xdiffs = as.matrix(data[trtnms,covars])
    xdiffs = xdiffs - as.matrix(data[ctrnms, covars])
    rowSums((xdiffs %*% inv.cov) * xdiffs)
}

dt_mahalo = as.data.frame(dt)
inv_cov_mat = solve(cov(dt_mahalo[,confounders]))
trtnms = row.names(dt_mahalo[as.logical(dt_mahalo$treat),])
ctrnms = row.names(dt_mahalo[!as.logical(dt_mahalo$treat),])
mahalo_dist = outer(trtnms,ctrnms,FUN = myMH, inv.cov = inv_cov_mat, data = dt_mahalo)

matches = apply(mahalo_dist,1, function(x) which(x == min(x)))
match_results = as.data.frame(cbind(seq(1,290), matches+290))
colnames(match_results) = c('trt.idx','ctr.idx')

counts = match_results %>% 
group_by(ctr.idx) %>%
dplyr::summarize(w_mahalo=n())

dt_mahalo_2 = dt_mahalo %>%
mutate(id = as.integer(rownames(dt_mahalo))) %>%
left_join(counts, by = c("id" = "ctr.idx")) %>%
mutate(w_mahalo = treat + ifelse(is.na(w_mahalo),0,w_mahalo))

dt$w_mahalo = dt_mahalo_2$w_mahalo

# Rematching
matches <- matching(z=dt$treat, score=dt$psc_probit, replace=TRUE)
dt[treat==1, w_probit:=1]
dt[treat==0, w_probit:=matches$cnts]

matches <- matching(z=dt$treat, score=dt$psc_full, replace=TRUE)
dt[treat==1, w_full:=1]
dt[treat==0, w_full:=matches$cnts]

matches <- matching(z=dt$treat, score=dt$psc_rf, replace=TRUE)
dt[treat==1, w_rf:=1]
dt[treat==0, w_rf:=matches$cnts]

matches <- matching(z=dt$treat, score=dt$psc_nn, replace=TRUE)
dt[treat==1, w_nn:=1]
dt[treat==0, w_nn:=matches$cnts]

# OVERLAP
# Paired, inverted histograms: propensity scores
nbins <- 40
examine_overlap(dt,40,var='psc_probit',var_name='Propensity Score (Probit Model)',ylim=c(-30,50),xlab='Propensity Score')
examine_overlap(dt,40,var='psc_full',var_name='Propensity Score (Probit Full Model)',ylim=c(-30,50),xlab='Propensity Score')
examine_overlap(dt,40,var='psc_rf',var_name='Propensity Score (RF Model)',ylim=c(-30,50),xlab='Propensity Score')
examine_overlap(dt,40,var='psc_nn',var_name='Propensity Score (NN Model)',ylim=c(-30,50),xlab='Propensity Score')

df = check_balance(dt, confounders, wts)
as.matrix(df)

df2 = check_balance(dt, confounders, dt$w_probit[291:1320])
as.matrix(df2)

df3 = check_balance(dt, confounders, dt$w_full[291:1320])
as.matrix(df3)

df4 = check_balance(dt, confounders, dt$w_rf[291:1320])
as.matrix(df4)

df5 = check_balance(dt, confounders, dt$w_nn[291:1320])
as.matrix(df5)

df6 = check_balance(dt, confounders, dt$w_mahalo[291:1320])
as.matrix(df6)
```


## Question 6: Repeat steps 2-4, but this time using IPTW.

\texttt{ANSWER:}
```{r Q6}
# Code for Estimating the Propensity Score
ps.m1 <- glm(treat ~ ., data=dt[,c('treat',confounders),with=F], family=binomial(link='logit'))
dt$psc <- ps.m1$fitted.values

# Code for Creating the IPTW Weights
trt = dt$treat==1
ctrl = dt$treat==0
```

For estimating the ATE, define the weight as follows:
$$\omega(Z,x) = \frac{Z}{\hat{\epsilon}(x)} + \frac{(1-Z)}{1-\hat{\epsilon(x)}}$$
```{r}
r_ate = dt$treat/dt$psc + (1-dt$treat)/(1-dt$psc)
```


For estimating the ATT, define the weight as follows:
$$\omega(Z,x) = Z + (1-Z) * \frac{\hat{\epsilon}(x)}{1-\hat{\epsilon}(x)}$$
```{r}
r_att = dt$psc + (1-dt$treat) * dt$psc / (1-dt$psc)
```

```{r}
iptw.balance <- check_balance(dt, confounders, r_att)
```

## Question 7: Comparative balance table
Create a table with columns 6 and 8 from your function for each of the matching and weighting methods performed above. Which approach would you choose and why? (1-2 paragraphs at most)


## Question 8: Estimate the treatment effect for the restructured datasets implied by Questions 4-6 (Step 5)


## Question 9: Assumptions


## Question 10: Causal Interpretation


## Question 11: Comparison to linear regression